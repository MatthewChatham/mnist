{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from common import get_mnist\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "\n",
    "mnist = get_mnist()\n",
    "xtrain = mnist['x']['train']\n",
    "ytrain = mnist['y']['train']\n",
    "xval = mnist['x']['val']\n",
    "yval = mnist['y']['val']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, Dropout, Activation, Dense\n",
    "from keras.regularizers import l2\n",
    "\n",
    "def evaluateOptimizer(opt='rmsprop'):\n",
    "    logistic = Sequential()\n",
    "    logistic.add(Dense(10, activation='softmax', input_shape=(784,), kernel_regularizer=l2(0.0001), activity_regularizer=l2(0.01)))\n",
    "    logistic.compile(\n",
    "        optimizer=opt,\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    logistic.fit((xtrain - xtrain.mean(axis=1).reshape(-1,1)) / xtrain.std(axis=1).reshape(-1,1), ytrain, epochs=5, batch_size=32)\n",
    "    return logistic.evaluate((xval - xval.mean(axis=1).reshape(-1,1)) / xval.std(axis=1).reshape(-1,1), yval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "55000/55000 [==============================] - 12s 222us/step - loss: 0.6499 - acc: 0.8830\n",
      "Epoch 2/5\n",
      "55000/55000 [==============================] - 10s 179us/step - loss: 0.5842 - acc: 0.9123\n",
      "Epoch 3/5\n",
      "55000/55000 [==============================] - 9s 170us/step - loss: 0.5765 - acc: 0.9173\n",
      "Epoch 4/5\n",
      "55000/55000 [==============================] - 10s 176us/step - loss: 0.5729 - acc: 0.9193\n",
      "Epoch 5/5\n",
      "55000/55000 [==============================] - 10s 188us/step - loss: 0.5713 - acc: 0.9197\n",
      "5000/5000 [==============================] - 0s 99us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5619612711906433, 0.9244]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluateOptimizer('rmsprop')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And there it is, our baseline Keras implementation of logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
